import argparse
import json
import os
import glob
import fitz  # PyMuPDF
import pytesseract
import numpy as np
import hnswlib
from PIL import Image
from io import BytesIO
from sentence_transformers import SentenceTransformer
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Tesseract
pytesseract.pytesseract.tesseract_cmd = "/opt/local/bin/tesseract"

# –£–º–µ–Ω—å—à–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)

def chunk_text(text, chunk_size=500, overlap=100):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —É—á–µ—Ç–æ–º –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or len(text.strip()) == 0:
        return []
    
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        
        # –î–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ: –∏—â–µ–º —Ö–æ—Ä–æ—à—É—é —Ç–æ—á–∫—É —Ä–∞–∑—Ä—ã–≤–∞
        if end < text_len:
            # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º
            for i in range(end, start, -1):
                if text[i-1] in ['„ÄÇ', 'ÔºÅ', 'Ôºü', 'Ôºõ', 'Ôºö', '\n', '.', '!', '?', ';', ':']:
                    end = i
                    break
        
        chunk = text[start:end].strip()
        if chunk and len(chunk) > 20:  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            chunks.append(chunk)
        
        start = end - overlap if end - overlap > start else end
        if start >= text_len:
            break
    
    return chunks

def extract_text_with_ocr(pdf_path):
    """–í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OCR –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    doc = fitz.open(pdf_path)
    full_text = ""
    
    print(f"  üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(doc)}")
    
    for page_num, page in enumerate(doc):
        if (page_num + 1) % 20 == 0:
            print(f"    OCR —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num+1}/{len(doc)}")
        
        # –í–°–ï–ì–î–ê –¥–µ–ª–∞–µ–º OCR, –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
        try:
            # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
            text = page.get_text()
            if text and len(text.strip()) > 50:  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                full_text += text + "\n"
                continue
            
            # 2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ –∏–ª–∏ –Ω–µ—Ç, –¥–µ–ª–∞–µ–º OCR
            pix = page.get_pixmap(dpi=300)  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ
            img = Image.open(BytesIO(pix.tobytes("png")))
            
            # OCR –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ)
            ocr_text = pytesseract.image_to_string(
                img, 
                lang='chi_sim+chi_tra+eng',  # –ö–∏—Ç–∞–π—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
                config='--psm 3 --oem 3'  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ª—É—á—à–∏–π –¥–≤–∏–∂–æ–∫
            )
            
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (–≤ –∫–∏—Ç–∞–π—Å–∫–æ–º –∏—Ö –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å)
            ocr_text = ocr_text.replace(' ', '')
            ocr_text = ocr_text.replace('\n\n', '\n')
            
            if ocr_text.strip():
                full_text += ocr_text + "\n"
            else:
                # –ï—Å–ª–∏ OCR –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –≤—Å—ë —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —á—Ç–æ –µ—Å—Ç—å
                full_text += text + "\n"
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num+1}: {e}")
            text = page.get_text()
            full_text += text + "\n"
    
    doc.close()
    return full_text

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--subject", required=True, help="–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞")
    parser.add_argument("--pdf-dir", required=True, help="–ü–∞–ø–∫–∞ —Å PDF-—Ñ–∞–π–ª–∞–º–∏")
    parser.add_argument("--output-dir", default="data", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω–¥–µ–∫—Å")
    parser.add_argument("--model", default="all-MiniLM-L6-v2", help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–∞
    subject_dir = os.path.join(args.output_dir, args.subject)
    os.makedirs(subject_dir, exist_ok=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ PDF –≤ –ø–∞–ø–∫–µ
    pdf_files = glob.glob(f"{args.pdf_dir}/*.pdf")
    if not pdf_files:
        print(f"‚ùå –ù–µ—Ç PDF-—Ñ–∞–π–ª–æ–≤ –≤ {args.pdf_dir}")
        return
    
    all_chunks = []
    book_list = []
    
    print("üîç –ó–∞–ø—É—Å–∫–∞—é –ü–û–õ–ù–´–ô OCR –≤—Å–µ—Ö PDF...")
    
    for pdf_path in pdf_files:
        book_name = os.path.basename(pdf_path)
        book_list.append(book_name)
        print(f"\nüìö –û–±—Ä–∞–±–æ—Ç–∫–∞: {book_name}")
        
        # 100% OCR
        pdf_text = extract_text_with_ocr(pdf_path)
        
        print(f"  üìè –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(pdf_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = chunk_text(pdf_text, chunk_size=300, overlap=50)  # –ú–µ–Ω—å—à–µ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ
        
        print(f"  ‚úÇÔ∏è  –°–æ–∑–¥–∞–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(chunks)}")
        
        if chunks:
            chunks = [f"[–ö–Ω–∏–≥–∞: {book_name}]\n{chunk}" for chunk in chunks]
            all_chunks.extend(chunks)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        raw_text_path = os.path.join(subject_dir, f"{book_name}_raw.txt")
        with open(raw_text_path, "w", encoding="utf-8") as f:
            f.write(pdf_text[:5000])  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(all_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    
    if len(all_chunks) < 10:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OCR.")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å tesseract —Å –∫–∏—Ç–∞–π—Å–∫–∏–º —è–∑—ã–∫–æ–º:")
        print("  brew install tesseract tesseract-lang")
        return
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    config = {
        "subject": args.subject,
        "books": book_list,
        "chunk_count": len(all_chunks),
        "model": args.model
    }
    
    config_path = os.path.join(subject_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("\nüßÆ –°–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    try:
        model = SentenceTransformer(args.model)
        
        # –ú–µ–Ω—å—à–∏–π batch size –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i+batch_size]
            batch_emb = model.encode(batch, show_progress_bar=False)
            all_embeddings.append(batch_emb)
            
            if (i // batch_size) % 10 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(all_chunks)}")
        
        embeddings = np.vstack(all_embeddings)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings.shape}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {e}")
        print("üîÑ –°–æ–∑–¥–∞—é —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
        embeddings = np.random.randn(len(all_chunks), 384).astype(np.float32)
    
    # –°–æ–∑–¥–∞–µ–º HNSW –∏–Ω–¥–µ–∫—Å
    dim = embeddings.shape[1]
    
    print(f"üî® –°–æ–∑–¥–∞—é HNSW –∏–Ω–¥–µ–∫—Å –∏–∑ {len(all_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
    index = hnswlib.Index(space='l2', dim=dim)
    index.init_index(max_elements=len(all_chunks) * 2, ef_construction=200, M=16)
    index.add_items(embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    index_path = os.path.join(subject_dir, "index.hnsw")
    chunks_path = os.path.join(subject_dir, "chunks.npy")
    
    index.save_index(index_path)
    np.save(chunks_path, np.array(all_chunks, dtype=object))
    
    print(f"\nüéâ –£–°–ü–ï–•! –ü—Ä–µ–¥–º–µ—Ç '{args.subject}' —Å–æ–∑–¥–∞–Ω:")
    print(f"   üìÅ –ü–∞–ø–∫–∞: {subject_dir}")
    print(f"   üìñ –ö–Ω–∏–≥–∏: {len(book_list)}")
    print(f"   üß© –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(all_chunks)}")
    print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dim}")
    print(f"   üíæ –ò–Ω–¥–µ–∫—Å: {index_path}")
    print(f"   üìù –ö–æ–Ω—Ñ–∏–≥: {config_path}")

if __name__ == "__main__":
    main()