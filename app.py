# app.py
import streamlit as st
import os
import json
import requests
import numpy as np
import hnswlib
from typing import List
import time
import tempfile
import fitz  # PyMuPDF
import glob
from sentence_transformers import SentenceTransformer
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç",
    page_icon="üìö",
    layout="wide"
)

# CSS —Å—Ç–∏–ª–∏
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 1rem;
    }
    .question-box {
        background-color: #F8FAFC;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #3B82F6;
        margin: 15px 0;
    }
    .answer-box {
        background-color: #F0F9FF;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #10B981;
        margin: 15px 0;
    }
    .warning-box {
        background-color: #FEF3C7;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #F59E0B;
        margin: 15px 0;
    }
</style>
""", unsafe_allow_html=True)

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
def chunk_text(text, chunk_size=300, overlap=50):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
    if not text or len(text.strip()) == 0:
        return []
    
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunk = text[start:end].strip()
        if chunk and len(chunk) > 20:
            chunks.append(chunk)
        start = end - overlap if end - overlap > start else end
    
    return chunks

def create_index_for_subject(subject_name, pdf_files):
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–∞"""
    import warnings
    warnings.filterwarnings('ignore')
    
    data_dir = "data"
    subject_dir = os.path.join(data_dir, subject_name)
    os.makedirs(subject_dir, exist_ok=True)
    
    all_chunks = []
    book_list = []
    
    # –ü—Ä–æ—Å—Ç–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
    for pdf_path in pdf_files:
        try:
            book_name = os.path.basename(pdf_path)
            book_list.append(book_name)
            
            doc = fitz.open(pdf_path)
            pdf_text = ""
            for page in doc:
                pdf_text += page.get_text() + "\n"
            doc.close()
            
            chunks = chunk_text(pdf_text)
            chunks = [f"[–ö–Ω–∏–≥–∞: {book_name}]\n{chunk}" for chunk in chunks]
            all_chunks.extend(chunks)
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_path}: {e}")
            continue
    
    if not all_chunks:
        return None
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    config = {
        "subject": subject_name,
        "books": book_list,
        "chunk_count": len(all_chunks)
    }
    
    config_path = os.path.join(subject_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)
    try:
        model = SentenceTransformer("all-MiniLM-L6-v2")
        embeddings = model.encode(all_chunks, show_progress_bar=False)
    except:
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = np.random.randn(len(all_chunks), 384).astype(np.float32)
    
    # –°–æ–∑–¥–∞–µ–º HNSW –∏–Ω–¥–µ–∫—Å
    dim = embeddings.shape[1]
    index = hnswlib.Index(space='l2', dim=dim)
    index.init_index(max_elements=len(all_chunks) * 2, ef_construction=200, M=16)
    index.add_items(embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    index_path = os.path.join(subject_dir, "index.hnsw")
    chunks_path = os.path.join(subject_dir, "chunks.npy")
    
    index.save_index(index_path)
    np.save(chunks_path, np.array(all_chunks, dtype=object))
    
    return {
        "config": config,
        "chunks_count": len(all_chunks),
        "index_path": index_path
    }

class SimpleEmbedder:
    def __init__(self, dim=384):
        self.dim = dim
    
    def encode(self, texts):
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = []
        for text in texts:
            seed = hash(text) % (2**32)
            np.random.seed(seed)
            emb = np.random.randn(self.dim).astype(np.float32)
            embeddings.append(emb)
        
        return np.array(embeddings)
    
    def get_sentence_embedding_dimension(self):
        return self.dim

class MultiSubjectTeacher:
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.model = SimpleEmbedder(dim=384)
        self.subjects = {}
        
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∏—Ö
        if not os.path.exists(data_dir) or not os.listdir(data_dir):
            self.create_default_indexes()
        else:
            self.load_all_subjects()
    
    def create_default_indexes(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
        st.info("üîÑ –°–æ–∑–¥–∞—é —É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_data = {
            "matan": [
                "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –µ—ë –∏–∑–º–µ–Ω–µ–Ω–∏—è.",
                "–ò–Ω—Ç–µ–≥—Ä–∞–ª - —ç—Ç–æ –æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –∫ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—é.",
                "–ü—Ä–µ–¥–µ–ª —Ñ—É–Ω–∫—Ü–∏–∏ –≤ —Ç–æ—á–∫–µ - —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –∫ –∫–æ—Ç–æ—Ä–æ–º—É —Å—Ç—Ä–µ–º–∏—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è.",
                "–†—è–¥ –¢–µ–π–ª–æ—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–ª–æ–∂–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω—É—é —Å—É–º–º—É.",
                "–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è."
            ],
            "linalg": [
                "–ú–∞—Ç—Ä–∏—Ü–∞ - —ç—Ç–æ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —á–∏—Å–µ–ª.",
                "–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –æ–±—Ä–∞—Ç–∏–º–∞ –ª–∏ –º–∞—Ç—Ä–∏—Ü–∞.",
                "–°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω–µ –º–µ–Ω—è—é—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏.",
                "–°–∏—Å—Ç–µ–º—ã –ª–∏–Ω–µ–π–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π —Ä–µ—à–∞—é—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –ì–∞—É—Å—Å–∞.",
                "–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ - —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏."
            ]
        }
        
        os.makedirs(self.data_dir, exist_ok=True)
        
        for subject_name, texts in test_data.items():
            subject_dir = os.path.join(self.data_dir, subject_name)
            os.makedirs(subject_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
            config = {
                "subject": subject_name,
                "books": ["—Ç–µ—Å—Ç–æ–≤—ã–π_—É—á–µ–±–Ω–∏–∫.pdf"],
                "chunk_count": len(texts)
            }
            
            config_path = os.path.join(subject_dir, "config.json")
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = np.random.randn(len(texts), 384).astype(np.float32)
            
            # –°–æ–∑–¥–∞–µ–º HNSW –∏–Ω–¥–µ–∫—Å
            index = hnswlib.Index(space='l2', dim=384)
            index.init_index(max_elements=len(texts) * 2, ef_construction=200, M=16)
            index.add_items(embeddings)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            index_path = os.path.join(subject_dir, "index.hnsw")
            chunks_path = os.path.join(subject_dir, "chunks.npy")
            
            index.save_index(index_path)
            np.save(chunks_path, np.array(texts, dtype=object))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ø–∞–º—è—Ç—å
            dim = self.model.get_sentence_embedding_dimension()
            index_loaded = hnswlib.Index(space='l2', dim=dim)
            index_loaded.load_index(index_path, max_elements=len(texts))
            
            self.subjects[subject_name] = {
                "config": config,
                "index": index_loaded,
                "chunks": np.array(texts, dtype=object)
            }
        
        st.success("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —Å–æ–∑–¥–∞–Ω—ã")
    
    def load_all_subjects(self):
        if not os.path.exists(self.data_dir):
            return
        
        for subject_name in os.listdir(self.data_dir):
            subject_path = os.path.join(self.data_dir, subject_name)
            if os.path.isdir(subject_path):
                try:
                    config_path = os.path.join(subject_path, "config.json")
                    if not os.path.exists(config_path):
                        continue
                    
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    index_path = os.path.join(subject_path, "index.hnsw")
                    chunks_path = os.path.join(subject_path, "chunks.npy")
                    
                    if not os.path.exists(index_path):
                        continue
                    
                    chunks = np.load(chunks_path, allow_pickle=True)
                    
                    dim = self.model.get_sentence_embedding_dimension()
                    index = hnswlib.Index(space='l2', dim=dim)
                    index.load_index(index_path, max_elements=len(chunks))
                    
                    self.subjects[subject_name] = {
                        "config": config,
                        "index": index,
                        "chunks": chunks
                    }
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {subject_name}: {e}")
    
    def detect_subject(self, question: str) -> List[str]:
        question_lower = question.lower()
        subject_keywords = {
            "matan": ["–º–∞—Ç–∞–Ω–∞–ª–∏–∑", "–º–∞—Ç –∞–Ω–∞–ª–∏–∑", "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª", "–∏–Ω—Ç–µ–≥—Ä–∞–ª", 
                     "–ø—Ä–µ–¥–µ–ª", "—Ä—è–¥", "—Ñ—É–Ω–∫—Ü–∏—è", "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è", "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ"],
            "linalg": ["–ª–∏–Ω–µ–π–Ω", "–º–∞—Ç—Ä–∏—Ü", "–≤–µ–∫—Ç–æ—Ä", "–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å", 
                      "—Å–æ–±—Å—Ç–≤–µ–Ω", "–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ", "–ª–∏–Ω–µ–π–Ω–æ", "–∞–ª–≥–µ–±—Ä"]
        }
        
        relevant_subjects = []
        for subject_name in self.subjects.keys():
            if subject_name in subject_keywords:
                for keyword in subject_keywords[subject_name]:
                    if keyword in question_lower:
                        if subject_name not in relevant_subjects:
                            relevant_subjects.append(subject_name)
                        break
        
        return relevant_subjects if relevant_subjects else list(self.subjects.keys())
    
    def search_in_subject(self, subject_name: str, query: str, top_k: int = 3):
        subject_data = self.subjects[subject_name]
        query_emb = self.model.encode([query])
        
        indices, distances = subject_data["index"].knn_query(query_emb, k=top_k)
        return [subject_data["chunks"][idx] for idx in indices[0]]
    
    def ask(self, question: str):
        if not self.subjects:
            return "‚ùå –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤."
        
        relevant_subjects = self.detect_subject(question)
        all_contexts = []
        
        for subject_name in relevant_subjects:
            try:
                chunks = self.search_in_subject(subject_name, question, top_k=3)
                subject_title = self.subjects[subject_name]["config"]["subject"]
                for chunk in chunks:
                    all_contexts.append(f"„Äê{subject_title}„Äë\n{chunk}")
            except:
                continue
        
        context = "\n\n".join(all_contexts)
        
        if context.strip():
            if len(context) > 6000:
                context = context[:6000] + "..."
            
            system_prompt = f"""
–¢—ã ‚Äî –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ.
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî –æ–±—ä—è—Å–Ω–∏ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.

–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ —É—á–µ–±–Ω–∏–∫–æ–≤:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç (–Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–æ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ø—Ä–æ—Å–∞):
"""
        else:
            system_prompt = f"""
–¢—ã ‚Äî –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –û–±—ä—è—Å–Ω—è–π —Ç–µ–º—ã –ø–æ–Ω—è—Ç–Ω–æ, –∫–∞–∫ –Ω–∞ –ª–µ–∫—Ü–∏–∏.

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:
"""
        
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if not api_key:
            return "‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ DEEPSEEK_API_KEY –≤ —Å–µ–∫—Ä–µ—Ç—ã Streamlit."
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "max_tokens": 2000,
            "temperature": 0.7
        }
        
        try:
            resp = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=120
            )
            
            if resp.status_code != 200:
                return f"‚ùå –û—à–∏–±–∫–∞ API: {resp.status_code}"
            
            data = resp.json()
            return data["choices"][0]["message"]["content"]
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
@st.cache_resource
def load_teacher():
    return MultiSubjectTeacher(data_dir="data")

def main():
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    st.markdown('<h1 class="main-header">üéì –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; color: #666;">AI-–ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–µ</p>', unsafe_allow_html=True)
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    with st.sidebar:
        st.image("https://cdn-icons-png.flaticon.com/512/2103/2103655.png", width=100)
        st.markdown("### üìö –û —Å–∏—Å—Ç–µ–º–µ")
        
        teacher = load_teacher()
        
        if teacher.subjects:
            st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–µ–¥–º–µ—Ç–æ–≤: {len(teacher.subjects)}")
            for subject_name, data in teacher.subjects.items():
                with st.expander(f"{data['config']['subject']}"):
                    st.write(f"üìñ –ö–Ω–∏–≥: {len(data['config']['books'])}")
                    st.write(f"üß© –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(data['chunks'])}")
        else:
            st.warning("‚ö†Ô∏è –ù–µ—Ç —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")
            if st.button("üîÑ –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"):
                teacher.create_default_indexes()
                st.rerun()
        
        st.markdown("---")
        st.markdown("### üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.caption("–î–ª—è —Ä–∞–±–æ—Ç—ã –Ω—É–∂–µ–Ω DeepSeek API –∫–ª—é—á")
        
        if not os.getenv('DEEPSEEK_API_KEY'):
            st.error("‚ùå DEEPSEEK_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
            st.info("–î–æ–±–∞–≤—å—Ç–µ –≤ Secrets Streamlit Cloud:")
            st.code("DEEPSEEK_API_KEY = sk-–≤–∞—à_–∫–ª—é—á")
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    st.markdown("### üí≠ –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        question = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ:",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: '–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è?' –∏–ª–∏ '–û–±—ä—è—Å–Ω–∏ –ø—Ä–∞–≤–∏–ª–æ –õ–æ–ø–∏—Ç–∞–ª—è'",
            height=100,
            key="question_input",
            label_visibility="collapsed"
        )
    
    with col2:
        st.markdown("### üí° –ü—Ä–∏–º–µ—Ä—ã")
        examples = ["–ß—Ç–æ —Ç–∞–∫–æ–µ –∏–Ω—Ç–µ–≥—Ä–∞–ª?", "–ö–∞–∫ –Ω–∞–π—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å?", "–û–±—ä—è—Å–Ω–∏ –º–µ—Ç–æ–¥ –ì–∞—É—Å—Å–∞"]
        for example in examples:
            if st.button(example, use_container_width=True):
                st.session_state.question = example
                st.rerun()
    
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        ask_button = st.button("üéØ –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç", type="primary", use_container_width=True, disabled=not question)
    with col_btn2:
        if st.button("üîÑ –û—á–∏—Å—Ç–∏—Ç—å", use_container_width=True):
            st.session_state.clear()
            st.rerun()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
    if ask_button and question:
        with st.spinner("üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —É—á–µ–±–Ω–∏–∫–∞—Ö..."):
            start_time = time.time()
            answer = teacher.ask(question)
            end_time = time.time()
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å
            st.markdown(f"""
            <div class="question-box">
            <strong>‚ùì –í–æ–ø—Ä–æ—Å:</strong><br>
            {question}
            </div>
            """, unsafe_allow_html=True)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            st.markdown(f"""
            <div class="answer-box">
            <strong>üìö –û—Ç–≤–µ—Ç:</strong><br>
            {answer}
            </div>
            """, unsafe_allow_html=True)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            st.caption(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {end_time-start_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            # –ö–Ω–æ–ø–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
            if st.button("üìã –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"):
                st.code(answer, language="markdown")
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –≤–æ–ø—Ä–æ—Å–∞
    if not question and not ask_button:
        st.markdown("---")
        st.markdown("""
        ### üìù –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
        1. –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –≤ –ø–æ–ª–µ –≤—ã—à–µ
        2. –ù–∞–∂–º–∏—Ç–µ "–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"
        3. –°–∏—Å—Ç–µ–º–∞ –Ω–∞–π–¥–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —É—á–µ–±–Ω–∏–∫–∞—Ö
        4. –ü–æ–ª—É—á–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç
        
        ### üéØ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–µ–º—ã:
        - **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:** –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ, –∏–Ω—Ç–µ–≥—Ä–∞–ª—ã, –ø—Ä–µ–¥–µ–ª—ã
        - **–õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞:** –º–∞—Ç—Ä–∏—Ü—ã, –≤–µ–∫—Ç–æ—Ä—ã, –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª–∏
        - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
        """)

if __name__ == "__main__":
    main()